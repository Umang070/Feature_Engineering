{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 0. 1.]\n",
      " [0. 1. 3.]]\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]        #sorted by feature name\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "\n",
    "\n",
    "print(v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}])\n",
    "\n",
    "v.transform({'foo': 4, 'unseen_feature': 3})  #unseen_feature is rejected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureHasher (Memory Efficient)\n",
    "Docstring:     \n",
    "Implements feature hashing, aka the hashing trick.\n",
    "\n",
    "This class turns sequences of symbolic feature names (strings) into\n",
    "scipy.sparse matrices, using a hash function to compute the matrix column\n",
    "corresponding to a name. The hash function employed is the signed 32-bit\n",
    "version of Murmurhash3.\n",
    "\n",
    "\n",
    "Feature names of type byte string are used as-is. Unicode strings are\n",
    "converted to UTF-8 first, but no Unicode normalization is done.\n",
    "Feature values must be (finite) numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -4.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          2.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         -2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -5.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "h = FeatureHasher(n_features=40)\n",
    "D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n",
    "f = h.transform(D)\n",
    "f.todense()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc', 'document', 'sec', 'second', 'umang']\n",
      "(4, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(['doc', 'sec', 'document'], dtype='<U8'),\n",
       " array(['second', 'document'], dtype='<U8'),\n",
       " array([], dtype='<U8'),\n",
       " array(['umang', 'document'], dtype='<U8')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the First document and sec doc.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one and .',\n",
    "    'Is this the first document? Umang',\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)\n",
    "# print(vectorizer.get_stop_words())          #--None\n",
    "vectorizer.inverse_transform(X)             #return terms per document with nonzero entries in X.\n",
    "# vectorizer.build_analyzer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "Methods to generate this mapping include neural networks       \n",
    "dimensionality reduction on the word co-occurrence matrix,               \n",
    "probabilistic models                 \n",
    "explainable knowledge base method            \n",
    "explicit representation in terms of the context in which words appear \n",
    "\n",
    "\n",
    "Software for training and using word embeddings includes Tomas Mikolov's Word2vec, Stanford University's GloVe, AllenNLP's Elmo,fastText, Gensim,Indra and Deeplearning4j. Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters.\n",
    "\n",
    "Thought Vector is extension of word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe: Global Vectors for Word Representation \n",
    "Introduction\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "1.   Nearest neighbors\n",
    "\n",
    "The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. Sometimes, the nearest neighbors according to this metric reveal rare but relevant words that lie outside an average human's vocabulary. For example, here are the closest words to the target word frog:\n",
    "\n",
    "2. 2.   Linear substructures\n",
    "\n",
    "The similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words.\n",
    "\n",
    "\n",
    "man - woman\n",
    "\n",
    "\n",
    "company - ceo\n",
    "\n",
    "\n",
    "city - zip code\n",
    "\n",
    "\n",
    "comparative - superlative\n",
    "\n",
    "The underlying concept that distinguishes man from woman, i.e. sex or gender, may be equivalently specified by various other word pairs, such as king and queen or brother and sister. To state this observation mathematically, we might expect that the vector differences man - woman, king - queen, and brother - sister might all be roughly equal. This property and other interesting patterns can be observed in the above set of visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Detection Features -- gensim : word2vec embedding\n",
    "\n",
    "\n",
    "sklearn_api.phrases – Scikit learn wrapper for phrase (collocation) detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\UMANG\n",
      "[nltk_data]     PATEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph = \"\"\"I have three vision for India.In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing the data\n",
    "# \\s stands for “whitespace character”. Again, which characters this actually includes, depends on the regex flavor. \n",
    "# In all flavors discussed in this tutorial, it includes [ \\t\\r\\n\\f].\n",
    "\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',paragraph)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)        #removing digits \n",
    "text = re.sub(r'\\s+',' ',text)        #removing white spaces\n",
    "# text = re.sub(r'[^a-z0-9]',' ',text)       #for removing special char\n",
    "# text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dr.', 0.35709553956985474),\n",
       " ('dhawan', 0.30520105361938477),\n",
       " ('gdp', 0.1781352311372757),\n",
       " ('father', 0.17399102449417114),\n",
       " ('vikram', 0.155681312084198),\n",
       " ('development', 0.1513759344816208),\n",
       " ('four', 0.14021986722946167),\n",
       " ('nurture', 0.1370331346988678),\n",
       " ('growth', 0.13159319758415222),\n",
       " ('portuguese', 0.12773531675338745)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)               #convert to sentences \n",
    " \n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]        #sentences to words\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]   #remove stop words\n",
    "    \n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)       #min in one document and it create 100 dimension vector by default\n",
    "\n",
    "\n",
    "words = model.wv.vocab               #vocabulary [unique words among all sentences ]\n",
    "\n",
    "\n",
    "# # Finding Word Vectors\n",
    "vector = model.wv['war']\n",
    "\n",
    "# # Most similar words\n",
    "similar = model.wv.most_similar('history')\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec is an efficient solution to these problems, which leverages the context of the target words. Essentially, we want to use the surrounding words to represent the target words with a Neural Network whose hidden layer encodes the word   representation.  \n",
    "There are two types of Word2Vec, Skip-gram and Continuous Bag of Words (CBOW). \n",
    "\n",
    "Skip-gram\n",
    "\n",
    "For skip-gram, the input is the target word, while the outputs are the words surrounding the target words. For instance, in the sentence “I have a cute dog”, the input would be “a”, whereas the output is “I”, “have”, “cute”, and “dog”, assuming the window size is 5. All the input and output data are of the same dimension and one-hot encoded. The network contains 1 hidden layer whose dimension is equal to the embedding size, which is smaller than the input/ output vector size.\n",
    "\n",
    "\n",
    "CBOW-Continous Bag of words\n",
    "\n",
    "It is similar to skip gram except swaps out i/p and o/p.          \n",
    "The biggest difference between Skip-gram and CBOW is that the way the word vectors are generated.           \n",
    "For CBOW, all the examples with the target word as target are fed into the networks, and taking the average of the extracted hidden layer.             \n",
    "For example, assume we only have two sentences, “He is a nice guy” and “She is a wise queen”.\n",
    "To compute the word representation for the word “a”, we need to feed in these two examples, “He is nice guy”, and “She is wise queen” into the Neural Network and take the average of the value in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TED talk Dataset, First, we download the the dataset using urllib, extracting the subtitle from the file\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the data\n",
    "# urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")\n",
    "# # extract subtitle\n",
    "# with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "#     doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "# input_text = '\\n'.join(doc.xpath('//content/text()'))\n",
    "# input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove parenthesis \n",
    "# input_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text)\n",
    "# # store as list of sentences\n",
    "# sentences_strings_ted = []\n",
    "# for line in input_text_noparens.split('\\n'):\n",
    "#     m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "#     sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "# # store as list of lists of words\n",
    "# sentences_ted = []\n",
    "# for sent_str in sentences_strings_ted:\n",
    "#     tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "#     sentences_ted.append(tokens)\n",
    "# sentences_ted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text)\n",
    "# sentences_string_ted = nltk.sent_tokenize(input_text_noparens)\n",
    "# sentences_ted_words  = [nltk.word_tokenize(sentence) for sentence in sentences_string_ted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_ted_words = []\n",
    "for sent_str in sentences_strings_ted:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()      #special symbol and punctuation marks(,.)\n",
    "    sentences_ted_words.append(tokens)\n",
    "# for i in range(len(sentences_ted_words)):\n",
    "#     sentences_ted_words[i] = [word for word in sentences_ted_words[i] if word not in stopwords.words('english')]   #remove stop words\n",
    "# sentences_ted_words[:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences: the list of split sentences.\n",
    "\n",
    "size: the dimensionality of the embedding vector\n",
    "\n",
    "window: the number of context words you are looking at\n",
    "\n",
    "min_count: tells the model to ignore words with total count less than this number.\n",
    "\n",
    "workers: the number of threads being used\n",
    "\n",
    "sg: whether to use skip-gram or CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted = Word2Vec(sentences=sentences_ted_words, size=100, window=5, min_count=5, workers=4, sg=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8511221408843994),\n",
       " ('guy', 0.8264572620391846),\n",
       " ('lady', 0.7827465534210205),\n",
       " ('girl', 0.7550215721130371),\n",
       " ('boy', 0.7485848665237427),\n",
       " ('gentleman', 0.7180063724517822),\n",
       " ('soldier', 0.7176931500434875),\n",
       " ('kid', 0.7104731202125549),\n",
       " ('poet', 0.6692278385162354),\n",
       " ('philosopher', 0.6434281468391418)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ted.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Word2Vec successfully handles the issue posed by one-hot vector, it has several limitation. The biggest challenge is that it is not able to represent words that do not appear in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "FastText is an extension to Word2Vec proposed by Facebook in 2016.Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words).         \n",
    "For instance, the tri-grams for the word apple is app, ppl, and ple (igno ring the starting and ending of boundaries of words).                 \n",
    "The word embedding vector for apple will be the sum of all these n-grams.                  \n",
    "After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset.       \n",
    "Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words.          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted = FastText(sentences_ted_words, size=100, window=2, min_count=10, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('audacious', 0.8744534254074097),\n",
       " ('dubious', 0.8740344047546387),\n",
       " ('ingenious', 0.8653150796890259),\n",
       " ('unambiguous', 0.8556569814682007),\n",
       " ('ludicrous', 0.8528010845184326),\n",
       " ('suspicious', 0.850383996963501),\n",
       " ('cautious', 0.8467897176742554),\n",
       " ('ambiguous', 0.8442314267158508),\n",
       " ('innocuous', 0.842936098575592),\n",
       " ('vicious', 0.8421852588653564)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ted.wv.most_similar(\"insidious\")   #rarely use word and not in trainig model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
