{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPEmb is a collection of pre-trained subword embeddings in 275 languages, based on Byte-Pair Encoding (BPE) and trained on Wikipedia. Its intended use is as input for neural models in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which vocabulary size should I pick?\n",
    "The vocabulary size is the sum of the number of BPE merge operations and the number of characters in the training data. The number of BPE merge operations determines if the resulting symbol sequences will tend to be short (few merge operations) or longer (more merge operations). Using very few merge operations will produce mostly character unigrams, bigrams, and trigrams, while peforming a large number of merge operations will create symbols representing the most frequent words. \n",
    "\n",
    "-->The advantage of having few operations is that this results in a smaller vocabulary of symbols. You need less data to learn representations (embeddings) of these symbols.          \n",
    "-->The disadvantage is that you need data to learn how to compose those symbols into meaningful units (e.g. words).\n",
    "\n",
    "-->The advantage of having many operations is that many frequent words get their own symbols, so you don't have to learn how what the word railway means by composing it from the symbols r, ail, and way.       \n",
    "-->The disadvantage is that you need more data to train good embeddings for these longer symbols, which is available for high-resource languages like English, but less so for low-resource languages like Khmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 400869/400869 [00:02<00:00, 144013.01B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d50.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1924908/1924908 [00:11<00:00, 172315.66B/s]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# load English BPEmb model with default vocabulary size (10k) and 50-dimensional embeddings\n",
    "bpemb_en = BPEmb(lang=\"en\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁ne', 'p', 'ot', 'ism']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpemb_en.encode(\"nepotism\")        #because we have only 10000 vocabulary size(few merge operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPEmb fallback: zh from vocab size 1000 to 10000\n",
      "downloading https://nlp.h-its.org/bpemb/zh/zh.wiki.bpe.vs10000.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/360798 [00:00<?, ?B/s]\u001b[A\n",
      "  3%|█▉                                                                        | 9216/360798 [00:00<00:07, 47420.48B/s]\u001b[A\n",
      "  9%|██████▊                                                                  | 33792/360798 [00:00<00:05, 59354.76B/s]\u001b[A\n",
      " 22%|███████████████▉                                                         | 78848/360798 [00:00<00:03, 74563.51B/s]\u001b[A\n",
      " 35%|█████████████████████████▎                                              | 126976/360798 [00:00<00:02, 95906.99B/s]\u001b[A\n",
      " 49%|██████████████████████████████████▍                                    | 175104/360798 [00:01<00:01, 113843.92B/s]\u001b[A\n",
      " 64%|█████████████████████████████████████████████▎                         | 230400/360798 [00:01<00:00, 137471.71B/s]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████▌              | 287744/360798 [00:01<00:00, 138490.05B/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████| 360798/360798 [00:01<00:00, 195954.46B/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/zh/zh.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                       | 0/3752816 [00:00<?, ?B/s]\u001b[A\n",
      "  0%|▏                                                                        | 9216/3752816 [00:00<01:17, 48380.55B/s]\u001b[A\n",
      "  1%|▋                                                                       | 33792/3752816 [00:00<01:01, 60628.47B/s]\u001b[A\n",
      "  2%|█▍                                                                      | 74752/3752816 [00:00<00:49, 75044.08B/s]\u001b[A\n",
      "  3%|██▎                                                                    | 123904/3752816 [00:00<00:37, 96864.86B/s]\u001b[A\n",
      "  5%|███▏                                                                  | 173056/3752816 [00:01<00:31, 114216.04B/s]\u001b[A\n",
      "  6%|████▏                                                                 | 222208/3752816 [00:01<00:25, 135865.29B/s]\u001b[A\n",
      "  7%|█████                                                                 | 271360/3752816 [00:01<00:22, 156403.73B/s]\u001b[A\n",
      "  9%|██████▏                                                               | 328704/3752816 [00:01<00:18, 180325.08B/s]\u001b[A\n",
      " 10%|███████▏                                                              | 386048/3752816 [00:01<00:16, 201759.97B/s]\u001b[A\n",
      " 12%|████████▎                                                             | 446464/3752816 [00:02<00:16, 200011.13B/s]\u001b[A\n",
      " 14%|█████████▌                                                            | 510976/3752816 [00:02<00:14, 224456.18B/s]\u001b[A\n",
      " 15%|██████████▋                                                           | 574464/3752816 [00:02<00:13, 244197.71B/s]\u001b[A\n",
      " 17%|███████████▉                                                          | 640000/3752816 [00:02<00:11, 263660.35B/s]\u001b[A\n",
      " 19%|█████████████▎                                                        | 713728/3752816 [00:02<00:10, 286478.62B/s]\u001b[A\n",
      " 21%|██████████████▋                                                       | 787456/3752816 [00:03<00:09, 305676.86B/s]\u001b[A\n",
      " 23%|████████████████                                                      | 862208/3752816 [00:03<00:08, 340796.07B/s]\u001b[A\n",
      " 25%|█████████████████▌                                                    | 943104/3752816 [00:03<00:08, 334602.65B/s]\u001b[A\n",
      " 28%|██████████████████▉                                                  | 1033216/3752816 [00:03<00:07, 377234.30B/s]\u001b[A\n",
      " 30%|████████████████████▋                                                | 1123328/3752816 [00:03<00:06, 377005.27B/s]\u001b[A\n",
      " 32%|██████████████████████▎                                              | 1214464/3752816 [00:04<00:06, 395290.72B/s]\u001b[A\n",
      " 35%|████████████████████████                                             | 1311744/3752816 [00:04<00:05, 415902.81B/s]\u001b[A\n",
      " 38%|██████████████████████████                                           | 1415168/3752816 [00:04<00:06, 348180.06B/s]\u001b[A\n",
      " 40%|███████████████████████████▉                                         | 1518592/3752816 [00:05<00:06, 345291.91B/s]\u001b[A\n",
      " 43%|█████████████████████████████▉                                       | 1631232/3752816 [00:05<00:06, 319565.15B/s]\u001b[A\n",
      " 47%|████████████████████████████████▏                                    | 1751040/3752816 [00:05<00:05, 371067.16B/s]\u001b[A\n",
      " 50%|██████████████████████████████████▍                                  | 1870848/3752816 [00:05<00:04, 403838.92B/s]\u001b[A\n",
      " 53%|████████████████████████████████████▊                                | 1999872/3752816 [00:06<00:03, 466511.14B/s]\u001b[A\n",
      " 57%|███████████████████████████████████████▎                             | 2135040/3752816 [00:06<00:03, 532697.20B/s]\u001b[A\n",
      " 61%|█████████████████████████████████████████▉                           | 2278400/3752816 [00:06<00:02, 610863.44B/s]\u001b[A\n",
      " 65%|████████████████████████████████████████████▌                        | 2425856/3752816 [00:06<00:01, 685656.26B/s]\u001b[A\n",
      " 68%|███████████████████████████████████████████████                      | 2558976/3752816 [00:06<00:01, 801376.59B/s]\u001b[A\n",
      " 71%|████████████████████████████████████████████████▊                    | 2654208/3752816 [00:06<00:01, 785845.15B/s]\u001b[A\n",
      " 74%|██████████████████████████████████████████████████▊                  | 2762752/3752816 [00:06<00:01, 853208.30B/s]\u001b[A\n",
      " 76%|████████████████████████████████████████████████████▌                | 2856960/3752816 [00:07<00:01, 861969.67B/s]\u001b[A\n",
      " 79%|██████████████████████████████████████████████████████▌              | 2966528/3752816 [00:07<00:00, 907828.48B/s]\u001b[A\n",
      " 83%|████████████████████████████████████████████████████████▉            | 3098624/3752816 [00:07<00:00, 956688.14B/s]\u001b[A\n",
      " 87%|██████████████████████████████████████████████████████████▊         | 3247104/3752816 [00:07<00:00, 1069411.03B/s]\u001b[A\n",
      " 90%|████████████████████████████████████████████████████████████▉       | 3361792/3752816 [00:07<00:00, 1068007.99B/s]\u001b[A\n",
      " 93%|███████████████████████████████████████████████████████████████▎    | 3491840/3752816 [00:07<00:00, 1126359.17B/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████| 3752816/3752816 [00:07<00:00, 482540.69B/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁', '这', '是一个', '中文', '句', '子']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpemb_zh = BPEmb(lang=\"zh\", vs=1000)        #we use very small merge operations\n",
    "# apply Chinese BPE subword segmentation model\n",
    "bpemb_zh.encode(\"这是一个中文句子\")  # \"This is a Chinese sentence.\"\n",
    "# ['▁这是一个', '中文', '句子']  # [\"This is a\", \"Chinese\", \"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ton', 0.9460763931274414),\n",
       " ('ston', 0.9455490112304688),\n",
       " ('bury', 0.9116721153259277),\n",
       " ('ington', 0.9063345193862915),\n",
       " ('well', 0.8849488496780396),\n",
       " ('▁chester', 0.8847955465316772),\n",
       " ('field', 0.8794451951980591),\n",
       " ('wick', 0.8759092092514038),\n",
       " ('ingham', 0.8693511486053467),\n",
       " ('worth', 0.8684747219085693)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings are wrapped in a gensim KeyedVectors object\n",
    "print(type(bpemb_en.emb))\n",
    "\n",
    "# You can use BPEmb objects like gensim KeyedVectors\n",
    "bpemb_en.most_similar(\"ford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bpemb_en.vectors)\n",
    "bpemb_en.vectors.shape               #10000 symbols and each has dimension 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140, 9929, 85, 679]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use subword embeddings in your neural network, either encode your input into subword IDs:\n",
    "\n",
    "ids = bpemb_en.encode_ids(\"nepotism\")\n",
    "print(ids)\n",
    "bpemb_en.vectors[ids].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Or use the embed method:\n",
    "# apply Chinese subword segmentation and perform embedding lookup\n",
    "bpemb_en.embed(\"nepotism\").shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech Tagging\n",
    "\n",
    "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,based on both its definition and its context. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, by a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also see how tagging is the second step in the typical NLP pipeline, following tokenization.\n",
    "\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The collection of tags used for a particular task is known as a tagset. Our emphasis in this chapter is on exploiting tags, and tagging text automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1   Using a Tagger\n",
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word (don't forget to import nltk):\n",
    "\n",
    " \t\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html   -->match shortform to part of speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\UMANG PATEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"And now for something which is completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that and is CC,--> a coordinating conjunction;    \n",
    "now and completely are RB, --> adverbs;    \n",
    "for is IN --> a preposition;     \n",
    "something is NN -->a noun;      \n",
    "different is JJ --> an adjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice that refuse and permit both appear as a present tense verb (VBP) and a noun (NN). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones). Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\UMANG\n",
      "[nltk_data]     PATEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n",
      "None\n",
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n",
      "None\n",
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    " \n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n",
      "None \n",
      "\n",
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n",
      "None \n",
      "\n",
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n",
      "None \n",
      "\n",
      "a his this their its her an that our any all one these my in your no\n",
      "some other and\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(text.similar('woman'),\"\\n\")  #noun\n",
    "\n",
    "print(text.similar('bought'),\"\\n\")  #verb\n",
    "print(text.similar('over'),\"\\n\")   #preposition\n",
    "print(text.similar('the'))       #determinere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\UMANG\n",
      "[nltk_data]     PATEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to C:\\Users\\UMANG\n",
      "[nltk_data]     PATEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Fulton', 'NOUN'), ...]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "nltk.download('treebank')\n",
    "print(nltk.corpus.brown.tagged_words(tagset='universal'))\n",
    "\n",
    "nltk.corpus.treebank.tagged_words(tagset='universal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to C:\\Users\\UMANG\n",
      "[nltk_data]     PATEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\indian.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM'), ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagged corpora for several other languages are distributed with NLTK, including Chinese, Hindi, Portuguese, Spanish, Dutch and Catalan. These usually contain non-ASCII text, and Python always displays this in hexadecimal when printing a larger structure such as a list.\n",
    "nltk.download('indian')\n",
    "\n",
    "# nltk.corpus.sinica_treebank.tagged_words()\n",
    "\n",
    "nltk.corpus.indian.tagged_words()\n",
    "\n",
    "# nltk.corpus.mac_morpho.tagged_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 30654),\n",
       " ('VERB', 14399),\n",
       " ('ADP', 12355),\n",
       " ('.', 11928),\n",
       " ('DET', 11389),\n",
       " ('ADJ', 6706),\n",
       " ('ADV', 3349),\n",
       " ('CONJ', 2717),\n",
       " ('PRON', 2535),\n",
       " ('PRT', 2264),\n",
       " ('NUM', 2166),\n",
       " ('X', 92)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories=\"news\",tagset=\"universal\")\n",
    "tag_freq = nltk.FreqDist(tag for (word,tag) in brown_news_tagged)\n",
    "tag_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508701792071921"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tag_freq.keys())\n",
    "sum(list(tag_freq.values())[:5])/sum(list(tag_freq.values()))   #75% words are tagged usiing most common 5 tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use POS (part of speech) features in scikit learn classfiers (SVM) etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Someone_NN should_MD have_VB this_DT ring_NN to_TO a_DT volcano_NN_umang']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we do this because [(have,VB),....] are then not fit in CountVectorize,TF-IDF\n",
    "\n",
    "text = nltk.word_tokenize('Someone should have this ring to a volcano')\n",
    "\n",
    "text_tagged = nltk.pos_tag(text)\n",
    "\n",
    "new_text = [word_tag[0] + \"_\"  +word_tag[1] for word_tag in text_tagged]\n",
    "doc_text = \" \".join(new_text)\n",
    "doc_text = [doc_text + \"_umang\"]\n",
    "doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_dt',\n",
       " 'have_vb',\n",
       " 'ring_nn',\n",
       " 'should_md',\n",
       " 'someone_nn',\n",
       " 'this_dt',\n",
       " 'to_to',\n",
       " 'volcano_nn_umang']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "dtm = cv.fit_transform(doc_text)\n",
    "dtm.toarray()\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
